# LLM Provider Configuration
#quarkus.langchain4j.chat-model.provider=ollama
quarkus.langchain4j.chat-model.provider=openai
quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}

# Ollama Configuration
langchain4j-ollama-dev-service.ollama.host=localhost
langchain4j-ollama-dev-service.ollama.port=11434
langchain4j-ollama-dev-service.ollama.endpoint=http://${langchain4j-ollama-dev-service.ollama.host}:${langchain4j-ollama-dev-service.ollama.port}
#quarkus.langchain4j.ollama.chat-model.model-id=gpt-oss:20b
#quarkus.langchain4j.ollama.chat-model.temperature=0.0
#quarkus.langchain4j.ollama.timeout=120s

# OpenAI Configuration (commented out)
# quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}
# quarkus.langchain4j.openai.chat-model.model-name=gpt-4o-mini

# Enable detailed logging
quarkus.langchain4j.log-requests=true
quarkus.langchain4j.log-responses=true
quarkus.log.level=INFO

# Server Configuration
quarkus.http.port=8888

# Logging configuration
%dev.quarkus.log.category."dev.langchain4j".level=DEBUG
%dev.quarkus.log.category."dev.langchain4j.agentic".level=DEBUG
%dev.quarkus.log.category."com.demo".level=DEBUG